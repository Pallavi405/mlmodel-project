{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692264f7",
   "metadata": {},
   "source": [
    "## NLP With Hotel Review Part 2\n",
    "\n",
    "### Pallavi Chintaluri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c46917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base packages. Other specific packages will be imported at the time of modelling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acd1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilise helper functions to enhance visualizations\n",
    "\n",
    "def PlotBoundaries(model, X, Y, dot_size=20, figsize=(10,7)) :\n",
    "    '''\n",
    "    Helper function that plots the decision boundaries of a model and data (X,Y)\n",
    "    code modified from: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "    '''\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1,X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "\n",
    "    #Plot\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=dot_size, edgecolor='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f754ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "\n",
    "clean_test_df = pd.read_csv(r'C:\\Users\\palla\\clean_data\\clean_test_dataframe.csv')\n",
    "clean_train_df = pd.read_csv(r'C:\\Users\\palla\\clean_data\\clean_train_dataframe.csv')\n",
    "\n",
    "# Seperate X and y variables for the two datasets\n",
    "\n",
    "# The training data is called remain data to facilitate train-validate split for later questions\n",
    "X_remain = clean_train_df.drop(columns = 'rating')\n",
    "y_remain = clean_train_df['rating']\n",
    "\n",
    "X_test = clean_test_df.drop(columns = 'rating')\n",
    "y_test = clean_test_df['rating']\n",
    "\n",
    "# Create train and validate sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = \\\n",
    "    train_test_split(X_remain, y_remain, test_size = 0.3,\n",
    "                     random_state=1)\n",
    "\n",
    "# Create a small sample set as well\n",
    "X_validate, X_sample, y_validate, y_sample = \\\n",
    "    train_test_split(X_validate, y_validate, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0db9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set: (4267, 2743)\n",
      "Shape of validation set: (1920, 2743)\n",
      "Shape of train set: (8958, 2743)\n",
      "Shape of train set: (1920, 2743)\n"
     ]
    }
   ],
   "source": [
    "# Look at the shapes of all our datasets\n",
    "\n",
    "print(f'Shape of test set: {X_test.shape}')\n",
    "print(f'Shape of validation set: {X_validate.shape}')\n",
    "print(f'Shape of train set: {X_train.shape}')\n",
    "print(f'Shape of train set: {X_sample.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39978e",
   "metadata": {},
   "source": [
    "Q1. Employ a linear classifier on this dataset:\n",
    "\n",
    " - Fit a logisitic regression model to this data with the solver set to lbfgs. What is the accuracy score on the test set?\n",
    " - What are the 20 words most predictive of a good review (from the positive review column)? What are the 20 words most predictive with a bad review (from the negative review column)? Use the regression coefficients to answer this question\n",
    " - Reduce the dimensionality of the dataset using PCA, what is the relationship between the number of dimensions and run-time for a logistic regression?\n",
    " - List one advantage and one disadvantage of dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "726e08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages for linear classification models\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f76857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12798, 2743)\n",
      "(2743, 2801)\n"
     ]
    }
   ],
   "source": [
    "# Use bagofwords to remove common English words from the train and test sets. \n",
    "# In this case we consider our original remain and test sets prior to creating validate and sample sets.\n",
    "\n",
    "bagofwords = CountVectorizer(stop_words=\"english\")\n",
    "bagofwords.fit(X_remain)\n",
    "\n",
    "X_remain_transformed = bagofwords.transform(X_remain) \n",
    "X_test_transformed = bagofwords.transform(X_test) \n",
    "\n",
    "# We can see how the dimensions have changed\n",
    "print(X_remain.shape)\n",
    "print(X_remain_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762acd9a",
   "metadata": {},
   "source": [
    "1a. Fit a logisitic regression model to this data with the solver set to lbfgs. What is the accuracy score on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bfea5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7225347710579778\n",
      "Test score: 0.7194750410124209\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model and setting the solver to lbfgs\n",
    "logreg = LogisticRegression(solver = 'lbfgs', C = 0.1) #Define the model\n",
    "logreg.fit(X_remain, y_remain) # Fit to remain dataset\n",
    "\n",
    "# Training and test accuracy scores\n",
    "print(f\"Train score: {logreg.score(X_remain, y_remain)}\") \n",
    "print(f\"Test score: {logreg.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ca013",
   "metadata": {},
   "source": [
    "We can see from the results above that our training data in this case has an accuracy score of **~72.3%** and the test data has an accuracy score of **~72%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023749d0",
   "metadata": {},
   "source": [
    "1b. What are the 20 words most predictive of a good review (from the positive review column)? What are the 20 words most predictive with a bad review (from the negative review column)? Use the regression coefficients to answer this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0cb57",
   "metadata": {},
   "source": [
    "1c. Reduce the dimensionality of the dataset using PCA, what is the relationship between the number of dimensions and run-time for a logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eb93eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset prior to reducing dimensions\n",
    "\n",
    "scaler = StandardScaler() # Define the scaler\n",
    "scaler.fit(X_remain) # Fit the scaler to the remain dataset\n",
    "X_remain = scaler.transform(X_remain)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f8d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we want to keep 90% of the variance\n",
    "my_PCA = PCA(n_components = 0.9) # Define model\n",
    "my_PCA.fit(X_remain) # Fit the remain dataset\n",
    "\n",
    "# Transform train and test\n",
    "X_remain_PCA = my_PCA.transform(X_remain)\n",
    "X_test_PCA = my_PCA.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1007a4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (12798, 2743)\n",
      "PCA Transformed: (12798, 1891)\n"
     ]
    }
   ],
   "source": [
    "print(f'Original: {X_remain.shape}')\n",
    "print(f'PCA Transformed: {X_remain_PCA.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246ffe3",
   "metadata": {},
   "source": [
    "We can see from this that the the dimensions have been reduced by around 850. Let us look at the impact on run time and accuracy of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ecf728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.8546647913736521\n",
      "Test Score: 0.7654089524255917\n"
     ]
    }
   ],
   "source": [
    "# Do the same but fit on the PCA transformed data\n",
    "my_logreg_PCA = LogisticRegression()\n",
    "\n",
    "# Fitting to PCA data\n",
    "my_logreg_PCA.fit(X_remain_PCA,y_remain)\n",
    "\n",
    "# Scoring on PCA train and test sets\n",
    "print(f'Train Score: {my_logreg_PCA.score(X_remain_PCA, y_remain)}')\n",
    "print(f'Test Score: {my_logreg_PCA.score(X_test_PCA, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bc8fb",
   "metadata": {},
   "source": [
    "We can see our accuracy scores have increased. The train set here has an accuracy of **~85.5%** and the test set has an accuracy score of **76.5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b2a4c04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.69 s ± 235 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logreg.fit(X_remain, y_remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e1b9f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_logreg_PCA.fit(X_remain_PCA,y_remain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80956de",
   "metadata": {},
   "source": [
    "We can see right away that the run time of the model with reduced dimensions is lower than the fulldataset. \n",
    "\n",
    "1d. List one advantage and one disadvantage of dimensionality reduction.\n",
    "\n",
    "This is one of the key advantages - **increased computational efficiency and decrease in resources**. \n",
    "However, the disadvantage of this approach is **loss of data**. We would lose much of the original dataset, but with PCA, elements of the original features are retained so it is preffered to simply dropping observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddeaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
