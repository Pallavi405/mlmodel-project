{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486d8e56",
   "metadata": {},
   "source": [
    "## NLP With Hotel Review Part 2\n",
    "\n",
    "### Pallavi Chintaluri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dac664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base packages. Other specific packages will be imported at the time of modelling\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d0627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilise helper functions to enhance visualizations\n",
    "\n",
    "def PlotBoundaries(model, X, Y, dot_size=20, figsize=(10,7)) :\n",
    "    '''\n",
    "    Helper function that plots the decision boundaries of a model and data (X,Y)\n",
    "    code modified from: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "    '''\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1,X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "\n",
    "    #Plot\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, s=dot_size, edgecolor='k')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91c3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "\n",
    "clean_test_df = pd.read_csv(r'C:\\Users\\palla\\clean_data\\clean_test_dataframe.csv')\n",
    "clean_train_df = pd.read_csv(r'C:\\Users\\palla\\clean_data\\clean_train_dataframe.csv')\n",
    "\n",
    "# Seperate X and y variables for the two datasets\n",
    "\n",
    "# The training data is called remain data to facilitate train-validate split for later questions\n",
    "X_remain = clean_train_df.drop(columns = 'rating')\n",
    "y_remain = clean_train_df['rating']\n",
    "\n",
    "X_test = clean_test_df.drop(columns = 'rating')\n",
    "y_test = clean_test_df['rating']\n",
    "\n",
    "# Create train and validate sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = \\\n",
    "    train_test_split(X_remain, y_remain, test_size = 0.3,\n",
    "                     random_state=1)\n",
    "\n",
    "# Create a small sample set as well\n",
    "X_validate, X_sample, y_validate, y_sample = \\\n",
    "    train_test_split(X_validate, y_validate, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c188a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set: (4267, 2743)\n",
      "Shape of validation set: (1920, 2743)\n",
      "Shape of train set: (8958, 2743)\n",
      "Shape of train set: (1920, 2743)\n"
     ]
    }
   ],
   "source": [
    "# Look at the shapes of all our datasets\n",
    "\n",
    "print(f'Shape of test set: {X_test.shape}')\n",
    "print(f'Shape of validation set: {X_validate.shape}')\n",
    "print(f'Shape of train set: {X_train.shape}')\n",
    "print(f'Shape of train set: {X_sample.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efc7d89",
   "metadata": {},
   "source": [
    "#### Q1. Employ a linear classifier on this dataset:\n",
    "\n",
    " - Fit a logisitic regression model to this data with the solver set to lbfgs. What is the accuracy score on the test set?\n",
    " - What are the 20 words most predictive of a good review (from the positive review column)? What are the 20 words most predictive with a bad review (from the negative review column)? Use the regression coefficients to answer this question\n",
    " - Reduce the dimensionality of the dataset using PCA, what is the relationship between the number of dimensions and run-time for a logistic regression?\n",
    " - List one advantage and one disadvantage of dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a95cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages for linear classification models\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "677c2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12798, 2743)\n",
      "(2743, 2801)\n"
     ]
    }
   ],
   "source": [
    "# Use bagofwords to remove common English words from the train and test sets. \n",
    "# In this case we consider our original remain and test sets prior to creating validate and sample sets.\n",
    "\n",
    "bagofwords = CountVectorizer(stop_words=\"english\")\n",
    "bagofwords.fit(X_remain)\n",
    "\n",
    "X_remain_transformed = bagofwords.transform(X_remain) \n",
    "X_test_transformed = bagofwords.transform(X_test) \n",
    "\n",
    "# We can see how the dimensions have changed\n",
    "print(X_remain.shape)\n",
    "print(X_remain_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72799212",
   "metadata": {},
   "source": [
    "1a. Fit a logisitic regression model to this data with the solver set to lbfgs. What is the accuracy score on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7a8c6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.7225347710579778\n",
      "Test score: 0.7194750410124209\n"
     ]
    }
   ],
   "source": [
    "# Fitting a model and setting the solver to lbfgs\n",
    "logreg = LogisticRegression(solver = 'lbfgs', C = 0.1) #Define the model\n",
    "logreg.fit(X_remain, y_remain) # Fit to remain dataset\n",
    "\n",
    "# Training and test accuracy scores\n",
    "print(f\"Train score: {logreg.score(X_remain, y_remain)}\") \n",
    "print(f\"Test score: {logreg.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479386a",
   "metadata": {},
   "source": [
    "We can see from the results above that our training data in this case has an accuracy score of **~72.3%** and the test data has an accuracy score of **~72%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92f644",
   "metadata": {},
   "source": [
    "1b. What are the 20 words most predictive of a good review (from the positive review column)? What are the 20 words most predictive with a bad review (from the negative review column)? Use the regression coefficients to answer this question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb72bf6",
   "metadata": {},
   "source": [
    "1c. Reduce the dimensionality of the dataset using PCA, what is the relationship between the number of dimensions and run-time for a logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcdca690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset prior to reducing dimensions\n",
    "\n",
    "scaler = StandardScaler() # Define the scaler\n",
    "scaler.fit(X_remain) # Fit the scaler to the remain dataset\n",
    "X_remain = scaler.transform(X_remain)  \n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f994aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets say we want to keep 90% of the variance\n",
    "my_PCA = PCA(n_components = 0.9) # Define model\n",
    "my_PCA.fit(X_remain) # Fit the remain dataset\n",
    "\n",
    "# Transform train and test\n",
    "X_remain_PCA = my_PCA.transform(X_remain)\n",
    "X_test_PCA = my_PCA.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04310628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (12798, 2743)\n",
      "PCA Transformed: (12798, 1891)\n"
     ]
    }
   ],
   "source": [
    "print(f'Original: {X_remain.shape}')\n",
    "print(f'PCA Transformed: {X_remain_PCA.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccb390",
   "metadata": {},
   "source": [
    "We can see from this that the the dimensions have been reduced by around 850. Let us look at the impact on run time and accuracy of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35912cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.8546647913736521\n",
      "Test Score: 0.7654089524255917\n"
     ]
    }
   ],
   "source": [
    "# Do the same but fit on the PCA transformed data\n",
    "my_logreg_PCA = LogisticRegression()\n",
    "\n",
    "# Fitting to PCA data\n",
    "my_logreg_PCA.fit(X_remain_PCA,y_remain)\n",
    "\n",
    "# Scoring on PCA train and test sets\n",
    "print(f'Train Score: {my_logreg_PCA.score(X_remain_PCA, y_remain)}')\n",
    "print(f'Test Score: {my_logreg_PCA.score(X_test_PCA, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0745aa6",
   "metadata": {},
   "source": [
    "We can see our accuracy scores have increased. The train set here has an accuracy of **~85.5%** and the test set has an accuracy score of **76.5%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "646a0988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.69 s ± 235 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "logreg.fit(X_remain, y_remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fdbabde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23 s ± 140 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "my_logreg_PCA.fit(X_remain_PCA,y_remain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5852843c",
   "metadata": {},
   "source": [
    "We can see right away that the run time of the model with reduced dimensions is lower than the fulldataset. \n",
    "\n",
    "1d. List one advantage and one disadvantage of dimensionality reduction.\n",
    "\n",
    "This is one of the key advantages - **increased computational efficiency and decrease in resources**. \n",
    "However, the disadvantage of this approach is **loss of data**. We would lose much of the original dataset, but with PCA, elements of the original features are retained so it is preffered to simply dropping observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cfa0f0",
   "metadata": {},
   "source": [
    "#### Q2. Employ a K-Nearest Neighbour classifier on this dataset:\n",
    "\n",
    " - Fit a KNN model to this data. What is the accuracy score on the test set?\n",
    " - KNN is a computationally expensive model. Reduce the number of observations (data points) in the dataset. What is the relationship between the number of observations and run-time for KNN?\n",
    " - List one advantage and one disadvantage of reducing the number of observations.\n",
    " - Use the dataset to find an optimal value for K in the KNN algorithm. You will need to split your dataset into train and validation sets.\n",
    " - What is the issue with splitting the data into train and validation sets after performing vectorization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8517dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for KNN models\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7edca6",
   "metadata": {},
   "source": [
    "2a. Fit a KNN model to this data. What is the accuracy score on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "797c1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 75.12%\n",
      "Test Accuracy: 63.67%\n"
     ]
    }
   ],
   "source": [
    "# 1. Fitting a KNN model to the remain dataset. Evaluate the model's accuracy.\n",
    "\n",
    "KNN_model = KNeighborsClassifier() # Define the model\n",
    "KNN_model.fit(X_remain, y_remain) # Fit to data\n",
    "\n",
    "print(f\"Train Accuracy: {round(KNN_model.score(X_remain, y_remain)*100, 2)}%\")\n",
    "print(f\"Test Accuracy: {round(KNN_model.score(X_test, y_test)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e8d8cc",
   "metadata": {},
   "source": [
    "We can see the accuracy on the train dataset for KNN models is **75.12%** and the accuracy of the test set is much lower at **63.67%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fc67a",
   "metadata": {},
   "source": [
    "2b. KNN is a computationally expensive model. Reduce the number of observations (data points) in the dataset. What is the relationship between the number of observations and run-time for KNN?\n",
    "\n",
    "For this case, we will use the sample dataset created at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67da193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 76.12%\n",
      "Test Accuracy: 57.6%\n"
     ]
    }
   ],
   "source": [
    "KNN_model = KNeighborsClassifier() # Define the model\n",
    "KNN_model.fit(X_train, y_train) # Fit to data\n",
    "\n",
    "print(f\"Train Accuracy: {round(KNN_model.score(X_train, y_train)*100, 2)}%\")\n",
    "print(f\"Test Accuracy: {round(KNN_model.score(X_test, y_test)*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "271bd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.6 ms ± 1.09 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "KNN_model.fit(X_remain, y_remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3740a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 ms ± 4.09 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "KNN_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af824a",
   "metadata": {},
   "source": [
    "We can see the computation time is not improved from simply dropping observations, however, we lose accuracy with the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22ce1b",
   "metadata": {},
   "source": [
    "2c. List one advantage and one disadvantage of reducing the number of observations.\n",
    "\n",
    "An advantage of reducing the number of observations is usually reduction in computational resources. However, unlike PCA we don't retain any elements of the original features. We simply lose datapoints for the model to learn from and understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f16ce67",
   "metadata": {},
   "source": [
    "2d. Use the dataset to find an optimal value for K in the KNN algorithm. You will need to split your dataset into train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899584e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb550db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best K value\n",
    "\n",
    "position_of_maximum_test_accuracy = np.argmax(test_acc)\n",
    "best_K = neighbors[position_of_maximum_test_accuracy]\n",
    "print(f\"Best K value is {best_K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad8bfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009cffe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
